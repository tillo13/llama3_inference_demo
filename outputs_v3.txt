(llama3_venv) PS F:\finetune> python .\fine_tune_llamav3.py
[INFO] Setting up Hugging Face cache...
[INFO] Logging into Hugging Face...
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to C:\Users\mac\.cache\huggingface\token
Login successful
[INFO] Login successful
[INFO] Loading dataset...
[INFO] Formatting dataset for chatbot...
Map: 100%|█████████████████████████████████████████████████████████████████████| 1522/1522 [00:00<00:00, 17298.44 examples/s]
[INFO] Dataset split into training and testing sets
[INFO] Loading tokenizer...
[INFO] Adding padding token to tokenizer...
[INFO] Loading model...
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████| 4/4 [00:09<00:00,  2.49s/it]
[INFO] Configuring LoRA...
[INFO] Preparing model for k-bit training...
F:\finetune\llama3_venv\Lib\site-packages\transformers\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
[INFO] Training arguments set. Output directory: F:/output_llama3_qlora
F:\finetune\llama3_venv\Lib\site-packages\huggingface_hub\utils\_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '1.0.0'.

Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
  warnings.warn(message, FutureWarning)
F:\finetune\llama3_venv\Lib\site-packages\trl\trainer\sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
  warnings.warn(
F:\finetune\llama3_venv\Lib\site-packages\trl\trainer\sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
  warnings.warn(
Map: 100%|██████████████████████████████████████████████████████████████████████| 1369/1369 [00:00<00:00, 7698.50 examples/s]
Map: 100%|████████████████████████████████████████████████████████████████████████| 153/153 [00:00<00:00, 7301.99 examples/s]
[INFO] Starting fine-tuning...
[INFO] Training started
  0%|                                                                                                | 0/342 [00:00<?, ?it/s]F:\finetune\llama3_venv\Lib\site-packages\torch\_dynamo\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
F:\finetune\llama3_venv\Lib\site-packages\transformers\models\llama\modeling_llama.py:660: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:555.)
  attn_output = torch.nn.functional.scaled_dot_product_attention(
F:\finetune\llama3_venv\Lib\site-packages\torch\utils\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
  3%|██▍                                                                                  | 10/342 [02:30<1:19:13, 14.32s/it][INFO] Epoch 0.05843681519357195: training_loss = 2.0477, eval_loss = N/A, learning_rate = 4.853801169590643e-05, time_taken = 150.7542176246643
{'loss': 2.0477, 'grad_norm': 0.0, 'learning_rate': 4.853801169590643e-05, 'epoch': 0.06}
  3%|██▍                                                                                  | 10/342 [02:30<1:19:13, 14.32s/it][INFO] Epoch 0.05843681519357195: training_loss = N/A, eval_loss = 2.022235870361328, learning_rate = N/A, time_taken = 245.71922039985657
{'eval_loss': 2.022235870361328, 'eval_runtime': 94.955, 'eval_samples_per_second': 1.611, 'eval_steps_per_second': 1.611, 'epoch': 0.06}
  6%|████▉                                                                                | 20/342 [06:34<1:19:47, 14.87s/it][INFO] Epoch 0.1168736303871439: training_loss = 2.0232, eval_loss = N/A, learning_rate = 4.707602339181287e-05, time_taken = 394.09489703178406
{'loss': 2.0232, 'grad_norm': 0.0, 'learning_rate': 4.707602339181287e-05, 'epoch': 0.12}
  6%|████▉                                                                                | 20/342 [06:34<1:19:47, 14.87s/it][INFO] Epoch 0.1168736303871439: training_loss = N/A, eval_loss = 2.022235870361328, learning_rate = N/A, time_taken = 489.15689754486084
{'eval_loss': 2.022235870361328, 'eval_runtime': 95.051, 'eval_samples_per_second': 1.61, 'eval_steps_per_second': 1.61, 'epoch': 0.12}
  9%|███████▍                                                                             | 30/342 [10:41<1:23:19, 16.03s/it][INFO] Epoch 0.17531044558071585: training_loss = 1.9325, eval_loss = N/A, learning_rate = 4.56140350877193e-05, time_taken = 641.2613916397095
{'loss': 1.9325, 'grad_norm': 0.0, 'learning_rate': 4.56140350877193e-05, 'epoch': 0.18}
  9%|███████▍                                                                             | 30/342 [10:41<1:23:19, 16.03s/it][INFO] Epoch 0.17531044558071585: training_loss = N/A, eval_loss = 2.022235870361328, learning_rate = N/A, time_taken = 736.3412709236145
{'eval_loss': 2.022235870361328, 'eval_runtime': 95.0689, 'eval_samples_per_second': 1.609, 'eval_steps_per_second': 1.609, 'epoch': 0.18}
 12%|█████████▉                                                                           | 40/342 [14:49<1:26:39, 17.22s/it][INFO] Epoch 0.2337472607742878: training_loss = 2.0043, eval_loss = N/A, learning_rate = 4.4152046783625734e-05, time_taken = 889.0653610229492
{'loss': 2.0043, 'grad_norm': 0.0, 'learning_rate': 4.4152046783625734e-05, 'epoch': 0.23}
 12%|█████████▉                                                                           | 40/342 [14:49<1:26:39, 17.22s/it][INFO] Epoch 0.2337472607742878: training_loss = N/A, eval_loss = 2.022235870361328, learning_rate = N/A, time_taken = 984.149379491806
{'eval_loss': 2.022235870361328, 'eval_runtime': 95.072, 'eval_samples_per_second': 1.609, 'eval_steps_per_second': 1.609, 'epoch': 0.23}
 15%|████████████▍                                                                        | 50/342 [18:55<1:19:36, 16.36s/it][INFO] Epoch 0.2921840759678597: training_loss = 1.9942, eval_loss = N/A, learning_rate = 4.269005847953216e-05, time_taken = 1135.6713137626648
{'loss': 1.9942, 'grad_norm': 0.0, 'learning_rate': 4.269005847953216e-05, 'epoch': 0.29}
 15%|████████████▍                                                                        | 50/342 [18:55<1:19:36, 16.36s/it][INFO] Epoch 0.2921840759678597: training_loss = N/A, eval_loss = 2.022235870361328, learning_rate = N/A, time_taken = 1230.765293598175
{'eval_loss': 2.022235870361328, 'eval_runtime': 95.083, 'eval_samples_per_second': 1.609, 'eval_steps_per_second': 1.609, 'epoch': 0.29}
 15%|████████████▍                                                                        | 50/342 [20:30<1:19:36, 16.36s/it]F:\finetune\llama3_venv\Lib\site-packages\torch\_dynamo\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
F:\finetune\llama3_venv\Lib\site-packages\torch\utils\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 18%|██████████████▉                                                                      | 60/342 [23:07<1:17:02, 16.39s/it][INFO] Epoch 0.3506208911614317: training_loss = 1.9956, eval_loss = N/A, learning_rate = 4.12280701754386e-05, time_taken = 1387.0669519901276
{'loss': 1.9956, 'grad_norm': 0.0, 'learning_rate': 4.12280701754386e-05, 'epoch': 0.35}
 18%|██████████████▉                                                                      | 60/342 [23:07<1:17:02, 16.39s/it][INFO] Epoch 0.3506208911614317: training_loss = N/A, eval_loss = 2.022235870361328, learning_rate = N/A, time_taken = 1482.1879570484161
{'eval_loss': 2.022235870361328, 'eval_runtime': 95.111, 'eval_samples_per_second': 1.609, 'eval_steps_per_second': 1.609, 'epoch': 0.35}
 20%|█████████████████▍                                                                   | 70/342 [27:18<1:15:17, 16.61s/it][INFO] Epoch 0.40905770635500366: training_loss = 1.9754, eval_loss = N/A, learning_rate = 3.976608187134503e-05, time_taken = 1638.204155921936
{'loss': 1.9754, 'grad_norm': 0.0, 'learning_rate': 3.976608187134503e-05, 'epoch': 0.41}
 20%|█████████████████▍                                                                   | 70/342 [27:18<1:15:17, 16.61s/it][INFO] Epoch 0.40905770635500366: training_loss = N/A, eval_loss = 2.022235870361328, learning_rate = N/A, time_taken = 1733.3171536922455
{'eval_loss': 2.022235870361328, 'eval_runtime': 95.103, 'eval_samples_per_second': 1.609, 'eval_steps_per_second': 1.609, 'epoch': 0.41}
 23%|███████████████████▉                                                                 | 80/342 [31:35<1:16:21, 17.49s/it][INFO] Epoch 0.4674945215485756: training_loss = 1.9035, eval_loss = N/A, learning_rate = 3.8304093567251465e-05, time_taken = 1895.2606408596039
{'loss': 1.9035, 'grad_norm': 0.0, 'learning_rate': 3.8304093567251465e-05, 'epoch': 0.47}
 23%|███████████████████▉                                                                 | 80/342 [31:35<1:16:21, 17.49s/it][INFO] Epoch 0.4674945215485756: training_loss = N/A, eval_loss = 2.022235870361328, learning_rate = N/A, time_taken = 1989.121744632721
{'eval_loss': 2.022235870361328, 'eval_runtime': 93.85, 'eval_samples_per_second': 1.63, 'eval_steps_per_second': 1.63, 'epoch': 0.47}
 26%|██████████████████████▎                                                              | 90/342 [35:46<1:12:02, 17.15s/it][INFO] Epoch 0.5259313367421475: training_loss = 1.8952, eval_loss = N/A, learning_rate = 3.6842105263157895e-05, time_taken = 2146.87979388237
{'loss': 1.8952, 'grad_norm': 0.0, 'learning_rate': 3.6842105263157895e-05, 'epoch': 0.53}
 26%|██████████████████████▎                                                              | 90/342 [35:46<1:12:02, 17.15s/it][INFO] Epoch 0.5259313367421475: training_loss = N/A, eval_loss = 2.022235870361328, learning_rate = N/A, time_taken = 2239.391951560974
{'eval_loss': 2.022235870361328, 'eval_runtime': 92.502, 'eval_samples_per_second': 1.654, 'eval_steps_per_second': 1.654, 'epoch': 0.53}
 29%|████████████████████████▌                                                           | 100/342 [39:50<1:05:59, 16.36s/it][INFO] Epoch 0.5843681519357194: training_loss = 1.9427, eval_loss = N/A, learning_rate = 3.538011695906433e-05, time_taken = 2390.5625751018524
{'loss': 1.9427, 'grad_norm': 0.0, 'learning_rate': 3.538011695906433e-05, 'epoch': 0.58}
 29%|████████████████████████▌                                                           | 100/342 [39:50<1:05:59, 16.36s/it][INFO] Epoch 0.5843681519357194: training_loss = N/A, eval_loss = 2.022235870361328, learning_rate = N/A, time_taken = 2483.1235840320587
{'eval_loss': 2.022235870361328, 'eval_runtime': 92.55, 'eval_samples_per_second': 1.653, 'eval_steps_per_second': 1.653, 'epoch': 0.58}
 29%|████████████████████████▌                                                           | 100/342 [41:23<1:05:59, 16.36s/it]F:\finetune\llama3_venv\Lib\site-packages\torch\_dynamo\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
F:\finetune\llama3_venv\Lib\site-packages\torch\utils\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 32%|███████████████████████████                                                         | 110/342 [44:02<1:02:31, 16.17s/it][INFO] Epoch 0.6428049671292915: training_loss = 1.9709, eval_loss = N/A, learning_rate = 3.391812865497076e-05, time_taken = 2642.2551214694977
{'loss': 1.9709, 'grad_norm': 0.0, 'learning_rate': 3.391812865497076e-05, 'epoch': 0.64}
 32%|███████████████████████████                                                         | 110/342 [44:02<1:02:31, 16.17s/it][INFO] Epoch 0.6428049671292915: training_loss = N/A, eval_loss = 2.022235870361328, learning_rate = N/A, time_taken = 2734.807240486145
{'eval_loss': 2.022235870361328, 'eval_runtime': 92.541, 'eval_samples_per_second': 1.653, 'eval_steps_per_second': 1.653, 'epoch': 0.64}
 35%|██████████████████████████████▏                                                       | 120/342 [47:57<56:27, 15.26s/it][INFO] Epoch 0.7012417823228634: training_loss = 1.9411, eval_loss = N/A, learning_rate = 3.24561403508772e-05, time_taken = 2877.3201982975006
{'loss': 1.9411, 'grad_norm': 0.0, 'learning_rate': 3.24561403508772e-05, 'epoch': 0.7}
 35%|██████████████████████████████▏                                                       | 120/342 [47:57<56:27, 15.26s/it][INFO] Epoch 0.7012417823228634: training_loss = N/A, eval_loss = 2.022235870361328, learning_rate = N/A, time_taken = 2969.862198114395
{'eval_loss': 2.022235870361328, 'eval_runtime': 92.531, 'eval_samples_per_second': 1.653, 'eval_steps_per_second': 1.653, 'epoch': 0.7}
 38%|████████████████████████████████▋                                                     | 130/342 [51:59<54:40, 15.47s/it][INFO] Epoch 0.7596785975164354: training_loss = 2.0046, eval_loss = N/A, learning_rate = 3.0994152046783626e-05, time_taken = 3119.14439702034
{'loss': 2.0046, 'grad_norm': 0.0, 'learning_rate': 3.0994152046783626e-05, 'epoch': 0.76}
 38%|████████████████████████████████▋                                                     | 130/342 [51:59<54:40, 15.47s/it][INFO] Epoch 0.7596785975164354: training_loss = N/A, eval_loss = 2.022235870361328, learning_rate = N/A, time_taken = 3211.7025067806244
{'eval_loss': 2.022235870361328, 'eval_runtime': 92.547, 'eval_samples_per_second': 1.653, 'eval_steps_per_second': 1.653, 'epoch': 0.76}
 41%|███████████████████████████████████▏                                                  | 140/342 [56:08<55:32, 16.50s/it][INFO] Epoch 0.8181154127100073: training_loss = 1.9557, eval_loss = N/A, learning_rate = 2.9532163742690062e-05, time_taken = 3368.648264169693
{'loss': 1.9557, 'grad_norm': 0.0, 'learning_rate': 2.9532163742690062e-05, 'epoch': 0.82}
 41%|███████████████████████████████████▏                                                  | 140/342 [56:08<55:32, 16.50s/it][INFO] Epoch 0.8181154127100073: training_loss = N/A, eval_loss = 2.022235870361328, learning_rate = N/A, time_taken = 3461.194266319275
{'eval_loss': 2.022235870361328, 'eval_runtime': 92.534, 'eval_samples_per_second': 1.653, 'eval_steps_per_second': 1.653, 'epoch': 0.82}
 44%|████████████████████████████████████▊                                               | 150/342 [1:00:06<51:22, 16.05s/it][INFO] Epoch 0.8765522279035792: training_loss = 1.9976, eval_loss = N/A, learning_rate = 2.8070175438596492e-05, time_taken = 3606.8816192150116
{'loss': 1.9976, 'grad_norm': 0.0, 'learning_rate': 2.8070175438596492e-05, 'epoch': 0.88}
 44%|████████████████████████████████████▊                                               | 150/342 [1:00:06<51:22, 16.05s/it][INFO] Epoch 0.8765522279035792: training_loss = N/A, eval_loss = 2.022235870361328, learning_rate = N/A, time_taken = 3699.451616048813
{'eval_loss': 2.022235870361328, 'eval_runtime': 92.558, 'eval_samples_per_second': 1.653, 'eval_steps_per_second': 1.653, 'epoch': 0.88}
 44%|████████████████████████████████████▊                                               | 150/342 [1:01:39<51:22, 16.05s/it]F:\finetune\llama3_venv\Lib\site-packages\torch\_dynamo\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
F:\finetune\llama3_venv\Lib\site-packages\torch\utils\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 47%|███████████████████████████████████████▎                                            | 160/342 [1:04:11<49:55, 16.46s/it][INFO] Epoch 0.9349890430971513: training_loss = 2.0732, eval_loss = N/A, learning_rate = 2.6608187134502928e-05, time_taken = 3852.0005507469177
{'loss': 2.0732, 'grad_norm': 0.0, 'learning_rate': 2.6608187134502928e-05, 'epoch': 0.93}
 47%|███████████████████████████████████████▎                                            | 160/342 [1:04:12<49:55, 16.46s/it][INFO] Epoch 0.9349890430971513: training_loss = N/A, eval_loss = 2.022235870361328, learning_rate = N/A, time_taken = 3944.554559469223
{'eval_loss': 2.022235870361328, 'eval_runtime': 92.543, 'eval_samples_per_second': 1.653, 'eval_steps_per_second': 1.653, 'epoch': 0.93}
 50%|█████████████████████████████████████████▊                                          | 170/342 [1:08:11<44:39, 15.58s/it][INFO] Epoch 0.9934258582907232: training_loss = 2.0498, eval_loss = N/A, learning_rate = 2.5146198830409358e-05, time_taken = 4091.446464538574
{'loss': 2.0498, 'grad_norm': 0.0, 'learning_rate': 2.5146198830409358e-05, 'epoch': 0.99}
 50%|█████████████████████████████████████████▊                                          | 170/342 [1:08:11<44:39, 15.58s/it][INFO] Epoch 0.9934258582907232: training_loss = N/A, eval_loss = 2.022235870361328, learning_rate = N/A, time_taken = 4184.016362667084
{'eval_loss': 2.022235870361328, 'eval_runtime': 92.5601, 'eval_samples_per_second': 1.653, 'eval_steps_per_second': 1.653, 'epoch': 0.99}
 53%|████████████████████████████████████████████▏                                       | 180/342 [1:12:09<43:52, 16.25s/it][INFO] Epoch 1.051862673484295: training_loss = 2.0678, eval_loss = N/A, learning_rate = 2.368421052631579e-05, time_taken = 4329.9426600933075
{'loss': 2.0678, 'grad_norm': 0.0, 'learning_rate': 2.368421052631579e-05, 'epoch': 1.05}
 53%|████████████████████████████████████████████▏                                       | 180/342 [1:12:09<43:52, 16.25s/it][INFO] Epoch 1.051862673484295: training_loss = N/A, eval_loss = 2.022235870361328, learning_rate = N/A, time_taken = 4422.487811088562
{'eval_loss': 2.022235870361328, 'eval_runtime': 92.533, 'eval_samples_per_second': 1.653, 'eval_steps_per_second': 1.653, 'epoch': 1.05}
 56%|██████████████████████████████████████████████▋                                     | 190/342 [1:16:11<40:22, 15.94s/it][INFO] Epoch 1.110299488677867: training_loss = 1.9126, eval_loss = N/A, learning_rate = 2.2222222222222223e-05, time_taken = 4571.486689090729
{'loss': 1.9126, 'grad_norm': 0.0, 'learning_rate': 2.2222222222222223e-05, 'epoch': 1.11}
 56%|██████████████████████████████████████████████▋                                     | 190/342 [1:16:11<40:22, 15.94s/it][INFO] Epoch 1.110299488677867: training_loss = N/A, eval_loss = 2.022235870361328, learning_rate = N/A, time_taken = 4664.024819612503
{'eval_loss': 2.022235870361328, 'eval_runtime': 92.527, 'eval_samples_per_second': 1.654, 'eval_steps_per_second': 1.654, 'epoch': 1.11}
 58%|█████████████████████████████████████████████████                                   | 200/342 [1:20:15<39:51, 16.84s/it][INFO] Epoch 1.1687363038714391: training_loss = 1.9685, eval_loss = N/A, learning_rate = 2.0760233918128656e-05, time_taken = 4815.146899938583
{'loss': 1.9685, 'grad_norm': 0.0, 'learning_rate': 2.0760233918128656e-05, 'epoch': 1.17}
 58%|█████████████████████████████████████████████████                                   | 200/342 [1:20:15<39:51, 16.84s/it][INFO] Epoch 1.1687363038714391: training_loss = N/A, eval_loss = 2.022235870361328, learning_rate = N/A, time_taken = 4907.669899463654
{'eval_loss': 2.022235870361328, 'eval_runtime': 92.512, 'eval_samples_per_second': 1.654, 'eval_steps_per_second': 1.654, 'epoch': 1.17}
 58%|█████████████████████████████████████████████████                                   | 200/342 [1:21:47<39:51, 16.84s/it]F:\finetune\llama3_venv\Lib\site-packages\torch\_dynamo\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
F:\finetune\llama3_venv\Lib\site-packages\torch\utils\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 61%|███████████████████████████████████████████████████▌                                | 210/342 [1:24:23<34:56, 15.88s/it][INFO] Epoch 1.227173119065011: training_loss = 1.9378, eval_loss = N/A, learning_rate = 1.929824561403509e-05, time_taken = 5063.6151003837585
{'loss': 1.9378, 'grad_norm': 0.0, 'learning_rate': 1.929824561403509e-05, 'epoch': 1.23}
 61%|███████████████████████████████████████████████████▌                                | 210/342 [1:24:23<34:56, 15.88s/it][INFO] Epoch 1.227173119065011: training_loss = N/A, eval_loss = 2.022235870361328, learning_rate = N/A, time_taken = 5156.154100418091
{'eval_loss': 2.022235870361328, 'eval_runtime': 92.527, 'eval_samples_per_second': 1.654, 'eval_steps_per_second': 1.654, 'epoch': 1.23}
 64%|██████████████████████████████████████████████████████                              | 220/342 [1:28:24<33:41, 16.57s/it][INFO] Epoch 1.285609934258583: training_loss = 2.1087, eval_loss = N/A, learning_rate = 1.7836257309941522e-05, time_taken = 5304.841501235962
{'loss': 2.1087, 'grad_norm': 0.0, 'learning_rate': 1.7836257309941522e-05, 'epoch': 1.29}
 64%|██████████████████████████████████████████████████████                              | 220/342 [1:28:24<33:41, 16.57s/it][INFO] Epoch 1.285609934258583: training_loss = N/A, eval_loss = 2.022235870361328, learning_rate = N/A, time_taken = 5397.382501125336
{'eval_loss': 2.022235870361328, 'eval_runtime': 92.53, 'eval_samples_per_second': 1.654, 'eval_steps_per_second': 1.654, 'epoch': 1.29}
 67%|████████████████████████████████████████████████████████▍                           | 230/342 [1:32:25<29:46, 15.95s/it][INFO] Epoch 1.3440467494521549: training_loss = 1.9725, eval_loss = N/A, learning_rate = 1.6374269005847955e-05, time_taken = 5545.89027094841
{'loss': 1.9725, 'grad_norm': 0.0, 'learning_rate': 1.6374269005847955e-05, 'epoch': 1.34}
 67%|████████████████████████████████████████████████████████▍                           | 230/342 [1:32:25<29:46, 15.95s/it][INFO] Epoch 1.3440467494521549: training_loss = N/A, eval_loss = 2.022235870361328, learning_rate = N/A, time_taken = 5638.442270517349
{'eval_loss': 2.022235870361328, 'eval_runtime': 92.54, 'eval_samples_per_second': 1.653, 'eval_steps_per_second': 1.653, 'epoch': 1.34}
 70%|██████████████████████████████████████████████████████████▉                         | 240/342 [1:36:29<28:29, 16.76s/it][INFO] Epoch 1.4024835646457268: training_loss = 2.0001, eval_loss = N/A, learning_rate = 1.4912280701754386e-05, time_taken = 5789.943917989731
{'loss': 2.0001, 'grad_norm': 0.0, 'learning_rate': 1.4912280701754386e-05, 'epoch': 1.4}
 70%|██████████████████████████████████████████████████████████▉                         | 240/342 [1:36:29<28:29, 16.76s/it][INFO] Epoch 1.4024835646457268: training_loss = N/A, eval_loss = 2.022235870361328, learning_rate = N/A, time_taken = 5882.496918439865
{'eval_loss': 2.022235870361328, 'eval_runtime': 92.542, 'eval_samples_per_second': 1.653, 'eval_steps_per_second': 1.653, 'epoch': 1.4}
 73%|█████████████████████████████████████████████████████████████▍                      | 250/342 [1:40:37<24:07, 15.73s/it][INFO] Epoch 1.4609203798392987: training_loss = 2.0305, eval_loss = N/A, learning_rate = 1.3450292397660819e-05, time_taken = 6037.601784229279
{'loss': 2.0305, 'grad_norm': 0.0, 'learning_rate': 1.3450292397660819e-05, 'epoch': 1.46}
 73%|█████████████████████████████████████████████████████████████▍                      | 250/342 [1:40:37<24:07, 15.73s/it][INFO] Epoch 1.4609203798392987: training_loss = N/A, eval_loss = 2.022235870361328, learning_rate = N/A, time_taken = 6130.127787590027
{'eval_loss': 2.022235870361328, 'eval_runtime': 92.515, 'eval_samples_per_second': 1.654, 'eval_steps_per_second': 1.654, 'epoch': 1.46}
 73%|█████████████████████████████████████████████████████████████▍                      | 250/342 [1:42:10<24:07, 15.73s/it]F:\finetune\llama3_venv\Lib\site-packages\torch\_dynamo\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
F:\finetune\llama3_venv\Lib\site-packages\torch\utils\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 76%|███████████████████████████████████████████████████████████████▊                    | 260/342 [1:44:52<24:07, 17.65s/it][INFO] Epoch 1.5193571950328706: training_loss = 1.8351, eval_loss = N/A, learning_rate = 1.1988304093567252e-05, time_taken = 6292.193143606186
{'loss': 1.8351, 'grad_norm': 0.0, 'learning_rate': 1.1988304093567252e-05, 'epoch': 1.52}
 76%|███████████████████████████████████████████████████████████████▊                    | 260/342 [1:44:52<24:07, 17.65s/it][INFO] Epoch 1.5193571950328706: training_loss = N/A, eval_loss = 2.022235870361328, learning_rate = N/A, time_taken = 6384.8061439991
{'eval_loss': 2.022235870361328, 'eval_runtime': 92.601, 'eval_samples_per_second': 1.652, 'eval_steps_per_second': 1.652, 'epoch': 1.52}
 79%|██████████████████████████████████████████████████████████████████▎                 | 270/342 [1:48:54<19:01, 15.85s/it][INFO] Epoch 1.5777940102264427: training_loss = 2.0043, eval_loss = N/A, learning_rate = 1.0526315789473684e-05, time_taken = 6534.894417524338
{'loss': 2.0043, 'grad_norm': 0.0, 'learning_rate': 1.0526315789473684e-05, 'epoch': 1.58}
 79%|██████████████████████████████████████████████████████████████████▎                 | 270/342 [1:48:54<19:01, 15.85s/it][INFO] Epoch 1.5777940102264427: training_loss = N/A, eval_loss = 2.022235870361328, learning_rate = N/A, time_taken = 6627.4915890693665
{'eval_loss': 2.022235870361328, 'eval_runtime': 92.585, 'eval_samples_per_second': 1.653, 'eval_steps_per_second': 1.653, 'epoch': 1.58}
 82%|████████████████████████████████████████████████████████████████████▊               | 280/342 [1:52:53<16:18, 15.79s/it][INFO] Epoch 1.6362308254200146: training_loss = 1.933, eval_loss = N/A, learning_rate = 9.064327485380117e-06, time_taken = 6773.51219534874
{'loss': 1.933, 'grad_norm': 0.0, 'learning_rate': 9.064327485380117e-06, 'epoch': 1.64}
 82%|████████████████████████████████████████████████████████████████████▊               | 280/342 [1:52:53<16:18, 15.79s/it][INFO] Epoch 1.6362308254200146: training_loss = N/A, eval_loss = 2.022235870361328, learning_rate = N/A, time_taken = 6866.1403114795685
{'eval_loss': 2.022235870361328, 'eval_runtime': 92.616, 'eval_samples_per_second': 1.652, 'eval_steps_per_second': 1.652, 'epoch': 1.64}
 85%|███████████████████████████████████████████████████████████████████████▏            | 290/342 [1:56:50<13:17, 15.34s/it][INFO] Epoch 1.6946676406135865: training_loss = 2.0148, eval_loss = N/A, learning_rate = 7.602339181286549e-06, time_taken = 7010.571599006653
{'loss': 2.0148, 'grad_norm': 0.0, 'learning_rate': 7.602339181286549e-06, 'epoch': 1.69}
 85%|███████████████████████████████████████████████████████████████████████▏            | 290/342 [1:56:50<13:17, 15.34s/it][INFO] Epoch 1.6946676406135865: training_loss = N/A, eval_loss = 2.022235870361328, learning_rate = N/A, time_taken = 7103.17370557785
{'eval_loss': 2.022235870361328, 'eval_runtime': 92.593, 'eval_samples_per_second': 1.652, 'eval_steps_per_second': 1.652, 'epoch': 1.69}
 88%|█████████████████████████████████████████████████████████████████████████▋          | 300/342 [2:00:50<11:07, 15.90s/it][INFO] Epoch 1.7531044558071585: training_loss = 2.0676, eval_loss = N/A, learning_rate = 6.140350877192982e-06, time_taken = 7250.822635650635
{'loss': 2.0676, 'grad_norm': 0.0, 'learning_rate': 6.140350877192982e-06, 'epoch': 1.75}
 88%|█████████████████████████████████████████████████████████████████████████▋          | 300/342 [2:00:50<11:07, 15.90s/it][INFO] Epoch 1.7531044558071585: training_loss = N/A, eval_loss = 2.022235870361328, learning_rate = N/A, time_taken = 7343.4046359062195
{'eval_loss': 2.022235870361328, 'eval_runtime': 92.571, 'eval_samples_per_second': 1.653, 'eval_steps_per_second': 1.653, 'epoch': 1.75}
 88%|█████████████████████████████████████████████████████████████████████████▋          | 300/342 [2:02:23<11:07, 15.90s/it]F:\finetune\llama3_venv\Lib\site-packages\torch\_dynamo\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
F:\finetune\llama3_venv\Lib\site-packages\torch\utils\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 91%|████████████████████████████████████████████████████████████████████████████▏       | 310/342 [2:04:53<08:22, 15.69s/it][INFO] Epoch 1.8115412710007304: training_loss = 2.0088, eval_loss = N/A, learning_rate = 4.678362573099415e-06, time_taken = 7493.869219303131
{'loss': 2.0088, 'grad_norm': 0.0, 'learning_rate': 4.678362573099415e-06, 'epoch': 1.81}
 91%|████████████████████████████████████████████████████████████████████████████▏       | 310/342 [2:04:53<08:22, 15.69s/it][INFO] Epoch 1.8115412710007304: training_loss = N/A, eval_loss = 2.022235870361328, learning_rate = N/A, time_taken = 7586.455218553543
{'eval_loss': 2.022235870361328, 'eval_runtime': 92.575, 'eval_samples_per_second': 1.653, 'eval_steps_per_second': 1.653, 'epoch': 1.81}
 94%|██████████████████████████████████████████████████████████████████████████████▌     | 320/342 [2:08:59<05:59, 16.33s/it][INFO] Epoch 1.8699780861943025: training_loss = 1.9008, eval_loss = N/A, learning_rate = 3.216374269005848e-06, time_taken = 7739.449376821518
{'loss': 1.9008, 'grad_norm': 0.0, 'learning_rate': 3.216374269005848e-06, 'epoch': 1.87}
 94%|██████████████████████████████████████████████████████████████████████████████▌     | 320/342 [2:08:59<05:59, 16.33s/it][INFO] Epoch 1.8699780861943025: training_loss = N/A, eval_loss = 2.022235870361328, learning_rate = N/A, time_taken = 7832.050389051437
{'eval_loss': 2.022235870361328, 'eval_runtime': 92.59, 'eval_samples_per_second': 1.652, 'eval_steps_per_second': 1.652, 'epoch': 1.87}
 96%|█████████████████████████████████████████████████████████████████████████████████   | 330/342 [2:12:59<03:16, 16.34s/it][INFO] Epoch 1.9284149013878744: training_loss = 2.0287, eval_loss = N/A, learning_rate = 1.7543859649122807e-06, time_taken = 7979.924173116684
{'loss': 2.0287, 'grad_norm': 0.0, 'learning_rate': 1.7543859649122807e-06, 'epoch': 1.93}
 96%|█████████████████████████████████████████████████████████████████████████████████   | 330/342 [2:12:59<03:16, 16.34s/it][INFO] Epoch 1.9284149013878744: training_loss = N/A, eval_loss = 2.022235870361328, learning_rate = N/A, time_taken = 8072.516173362732
{'eval_loss': 2.022235870361328, 'eval_runtime': 92.58, 'eval_samples_per_second': 1.653, 'eval_steps_per_second': 1.653, 'epoch': 1.93}
 99%|███████████████████████████████████████████████████████████████████████████████████▌| 340/342 [2:17:06<00:32, 16.37s/it][INFO] Epoch 1.9868517165814463: training_loss = 1.9367, eval_loss = N/A, learning_rate = 2.9239766081871344e-07, time_taken = 8226.875396966934
{'loss': 1.9367, 'grad_norm': 0.0, 'learning_rate': 2.9239766081871344e-07, 'epoch': 1.99}
 99%|███████████████████████████████████████████████████████████████████████████████████▌| 340/342 [2:17:06<00:32, 16.37s/it][INFO] Epoch 1.9868517165814463: training_loss = N/A, eval_loss = 2.022235870361328, learning_rate = N/A, time_taken = 8321.796394348145
{'eval_loss': 2.022235870361328, 'eval_runtime': 94.911, 'eval_samples_per_second': 1.612, 'eval_steps_per_second': 1.612, 'epoch': 1.99}
100%|████████████████████████████████████████████████████████████████████████████████████| 342/342 [2:19:11<00:00, 35.48s/it][INFO] Epoch 1.9985390796201608: training_loss = N/A, eval_loss = N/A, learning_rate = N/A, time_taken = 8357.019297838211
{'train_runtime': 8357.0243, 'train_samples_per_second': 0.328, 'train_steps_per_second': 0.041, 'train_loss': 1.9838783573686032, 'epoch': 2.0}
100%|████████████████████████████████████████████████████████████████████████████████████| 342/342 [2:19:17<00:00, 24.44s/it]
[INFO] Fine-tuning completed in 8357.57 seconds
[INFO] Saving the fine-tuned model...
[INFO] Training results saved
[INFO] Running inference to evaluate the fine-tuned model...
The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
F:\finetune\llama3_venv\Lib\site-packages\torch\_dynamo\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
F:\finetune\llama3_venv\Lib\site-packages\torch\utils\checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
[INFO] Inference result:
User: I'm feeling really anxious about my upcoming exams. Assistant: I'm sorry to hear that. How can I help?
User: I'm feeling really anxious about my upcoming exams. Assistant: I'm sorry to hear that. How can I help?
[INFO] Inference result saved
[INFO] Plotting training metrics...
[INFO] Training and evaluation metrics plot saved to F:/output_llama3_qlora/training_metrics.png

=== SUMMARY ===
Total script execution time: 8393.91 seconds
Fine-tuning time: 8357.57 seconds
Output directory: F:/output_llama3_qlora
Number of training examples: 1369
Number of evaluation examples: 153

=== FINAL TRAINING METRICS ===
Epoch: 1.9985390796201608
Training Loss: nan
Evaluation Loss: nan
Learning Rate: nan
Time Taken: 8357.02 seconds